@startuml
!pragma teoz true
skinparam shadowing false
' skinparam monochrome true
' hide footbox
hide empty members


SupervisedModel <|-- NeuralNetwork


Layer <|-- ClassicLayer
Layer <|-- ConvolutionLayer
Layer <|-- PoolingLayer
Layer <|-- Recurrent

PoolingLayer <|-- MaxPooling
PoolingLayer <|-- AvgPooling

Recurrent <|-- LSTM
Recurrent <|-- SimpleRNN


Layer --> "1" InitializationFunction: initFunction >



InitializationFunction <|-- Random
InitializationFunction <|-- He
InitializationFunction <|-- Xavier
InitializationFunction <|-- Other



NeuralNetwork -- "1" CostFunction : costFunction >
NeuralNetwork --> "*" Layer : hiddenLayers >

Layer --> "1" ActivationFunction : activationFunction >


CostFunction <|-- MeanAbsoluteError
CostFunction <|-- MeanSquaredError
CostFunction <|-- CrossEntropy
CostFunction <|-- BinaryCrossEntropy

ActivationFunction <|-- ReLU
ActivationFunction <|-- Sigmoid
ActivationFunction <|-- TanH
ActivationFunction <|-- Softmax


Layer <|-- Dropout

SupervisedModel <|-- BaggingRegularization
BaggingRegularization --> "1..*" SupervisedModel : models >



NeuralNetwork --> "0..1" NormPenaltyRegularization: regularizationFunction >



NormPenaltyRegularization <|-- L1WeightDecay
NormPenaltyRegularization <|-- L2WeightDecay
' CostFunction --> "0..1" NormPenaltyRegularization : regularizationFunction >



NeuralNetwork --> "1" BatchFunction: batchFunction >

BatchFunction <|-- BatchMode
BatchFunction <|-- MiniBatch
MiniBatch <|-- MiniBatchNormalized


abstract class SupervisedModel {
      {abstract} + train(input: Vector, expectedOutput: Vector): void
      {abstract} + predict(input: Vector): Vector
}

class NeuralNetwork {
      - num_inputs: int
      - num_ouputs: int
      - learning_rate: float
      - iterations: int
      - regularization_rate: float

      + train(input: Vector, expectedOutput: Vector, batch_function: BatchFunction): void
      + backprop(output, expected): Tuple[List[Matrix],List[Vector]]
}

abstract class CostFunction {
      {abstract} + calculate(value, expected_value): float
      {abstract} + calculateGradient(value, expected_value): Vector
}

class MeanAbsoluteError
class MeanSquaredError
class CrossEntropy
class BinaryCrossEntropy

abstract class Layer {
      - bias: Vector
      - weights: Matrix
      - units: int
      
      {abstract} + forward(input: Vector): Vector
      {abstract} + backward(gradient: Vector, learning_rate: float): Vector
      {abstract} + update_weight(learning_rate: float, grads: Vector): void
      {abstract} + update_bias(learning_rate: float, grads: Vector): void
}

class ClassicLayer
class ConvolutionLayer
class Dropout

abstract class Recurrent
class LSTM
class SimpleRNN

abstract class PoolingLayer
class MaxPooling
class AvgPooling


abstract class ActivationFunction {
      {abstract} + calculate(...): Vector
      {abstract} + calculateGradient(...): Vector
}

abstract class InitializationFunction {
	 {abstract} + initialize(x: int, y: int): Matrix
}

class Random
class He
class Xavier
class Other


class BaggingRegularization {
      + _(models: List[SupervizedModels])
}

abstract class NormPenaltyRegularization {
	 {abstract} + calculate(layer: Layer): float
	 {abstract} + calculateGradientWeight(layer: Layer): Matrix
	 {abstract} + calculateGradientBiase(layer: Layer): Vector
}

class L1WeightDecay
class L2WeightDecay

abstract class BatchFunction {
	 {abstract} + _(input_data: Matrix, expected_output: Matrix): void
	 {abstract} + get_batch(): Tuple[Matrix, Matrix]
}

class BatchMode
class MiniBatch

class MiniBatchNormalized {
      {static} - epsilon: float
}

abstract class Optimizer
class Adam
class AdaGrad
class RMSProp

NeuralNetwork --> "1" Optimizer

Optimizer <|-- Adam
Optimizer <|-- RMSProp
Optimizer <|-- AdaGrad
Optimizer <|-- SGD
Optimizer <|-- SGDMomentum

@enduml