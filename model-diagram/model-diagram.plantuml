@startuml
!pragma teoz true
' skinparam shadowing false
' skinparam monochrome true
' hide footbox
hide empty members

' CostFunctions #01c472
' BatchFunctions #c15360
' Optimizers #528e8c
' RecurrentLayer #e91451
' InitializationFunctions #8da83e
' ConvolutionLayers #6865f0
' ActivationFunctions #c4a499
' Regularization #8b9bdc

SupervisedModel <|-- NeuralNetwork


Layer <|-- ClassicLayer


package RecurrentLayer {
abstract class Recurrent

Layer <|-- Recurrent

Recurrent <|-- LSTM
Recurrent <|-- SimpleRNN
}

package InitializationFunctions {
Layer --> "1" InitializationFunction: initFunction >

abstract class InitializationFunction {
	{abstract} + initialize(x: int, y: int): Matrix
}

InitializationFunction <|-- Random
InitializationFunction <|-- He
InitializationFunction <|-- Xavier
InitializationFunction <|-- He2

}

NeuralNetwork --> "*" Layer : hiddenLayers >

package CostFunctions {
NeuralNetwork -- "1" CostFunction : costFunction >


abstract class CostFunction  {
      {abstract} + calculate(value: Vector, expectedValue: Vector): Vector
      {abstract} + calculate_gradient(value: Vector, expectedValue: Vector): Vector
}

CostFunction <|-- MeanAbsoluteError
CostFunction <|-- MeanSquaredError
CostFunction <|-- CrossEntropy
CostFunction <|-- BinaryCrossEntropy
}



SupervisedModel <|-- BaggingRegularization
' CostFunction --> "0..1" NormPenaltyRegularization : regularizationFunction >


abstract class SupervisedModel {
      {abstract} + train(input: Vector, expectedOutput: Vector, batchFunction: BatchFunction): void
      {abstract} + predict(input: Vector): Vector
}

class NeuralNetwork {
      - layers: Layer[]
      - costFunction: CostFunction
      - learning_rate: float
      - epochs: int
      - regularization_function: NormRegularizationFunction
      - optimizer: Optimizer

      - backprop(output, expected): void
}

abstract class Layer {
      - bias: Vector
      - weights: Matrix
      - units: int
      - optimizer: Optimizer
      
      {abstract} + forward(input: Vector): Vector
      {abstract} + backward(gradient: Vector, learningRate: float): Vector
      {abstract} + update_weight(learningRate: float, grads: Vector): void
      {abstract} + update_bias(learningRate: float, grads: Vector): void
      + set_optimizer(optimizer: Optimizer): void

}

package ConvolutionLayers {
class Convolution2DLayer {
      - channels: int
      - filterSize: int
      - filters: Tensor
}

class UnflattenLayer {
      - width: int
      - height: int
}

abstract class PoolingLayer {
      - poolSize: [int, int]
      - stride: int
      ' - padding: Padding
}
Layer <|-- Convolution2DLayer
Layer <|-- PoolingLayer

abstract class ReshapeLayer

Layer <|-- ReshapeLayer

ReshapeLayer <|-- FlattenLayer
ReshapeLayer <|-- UnflattenLayer

PoolingLayer <|-- MaxPooling
PoolingLayer <|-- AvgPooling
}

package ActivationFunctions {
abstract class ActivationFunction {
      {abstract} + calculate(value: Vector): Vector
      {abstract} + calculate_gradient(value: Vector): Vector
}

Layer --> "0..1" ActivationFunction : activationFunction >

ActivationFunction <|-- ReLU
ActivationFunction <|-- Sigmoid
ActivationFunction <|-- TanH
ActivationFunction <|-- Softmax
}

package Regularization {
class BaggingRegularization {
      + _(models: List[SupervizedModels])
}

BaggingRegularization --> "1..*" SupervisedModel : models >

abstract class NormPenaltyRegularization {
      - regularizationRate: float

	 {abstract} + calculate(layer: Layer): float
	 {abstract} + calculate_gradient_weight(layer: Layer): Matrix
	 {abstract} + calculate_gradient_bias(layer: Layer): Vector
}

NeuralNetwork --> "0..1" NormPenaltyRegularization: regularizationFunction >

NormPenaltyRegularization <|-- L1WeightDecay
NormPenaltyRegularization <|-- L2WeightDecay

Layer <|-- Dropout

class Dropout {
      - dropoutRate: float
}
}

package BatchFunctions {
abstract class BatchFunction {
      - inputData: Matrix
      - expectedOutput: Matrix

	 {abstract} + get_batch(): Tuple[Matrix, Matrix]
}

class BatchMode
class MiniBatch

class MiniBatchNormalized {
      {static} - epsilon: float
}

NeuralNetwork --> "1" BatchFunction: batchFunction >

BatchFunction <|-- BatchMode
BatchFunction <|-- MiniBatch
MiniBatch <|-- MiniBatchNormalized
}

package Optimizers {
abstract class Optimizer{
      + calculate_weight(grad: Vector, learningRate: float):
      + calculate_bias(grad: Vector, learningRate: float):
}

NeuralNetwork --> "1" Optimizer

Optimizer <|-- Adam
Optimizer <|-- RMSProp
Optimizer <|-- AdaGrad
Optimizer <|-- SGD
Optimizer <|-- SGDMomentum
}

abstract class Callback {
      + call(metrics_history: List[])
}

Callback <|-- GraphicCallback

abstract class Metric {
      + calculate(predicted: Vector, expected: Vector): float
}

Metric <|-- MseMetric
Metric <|-- MaeMetric

NeuralNetwork --> "*" Callback: "callbacks"
NeuralNetwork --> "*" Metric: "metrics"

@enduml