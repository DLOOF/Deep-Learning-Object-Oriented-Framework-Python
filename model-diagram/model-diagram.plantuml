@startuml
!pragma teoz true
' skinparam shadowing false
' skinparam monochrome true
' hide footbox
hide empty members

' CostFunctions #01c472
' BatchFunctions #c15360
' Optimizers #528e8c
' RecurrentLayer #e91451
' InitializationFunctions #8da83e
' ConvolutionLayers #6865f0
' ActivationFunctions #c4a499
' Regularization #8b9bdc

SupervisedModel <|-- NeuralNetwork


Layer <|-- ClassicLayer


package RecurrentLayer {
abstract class Recurrent

Layer <|-- Recurrent

Recurrent <|-- LSTM
Recurrent <|-- SimpleRNN
}

package InitializationFunctions {
Layer --> "1" InitializationFunction: initFunction >

abstract class InitializationFunction {
	{abstract} + initialize(x: int, y: int): Matrix
}

InitializationFunction <|-- Random
InitializationFunction <|-- He
InitializationFunction <|-- Xavier
InitializationFunction <|-- Other

}

NeuralNetwork --> "*" Layer : hiddenLayers >

package CostFunctions {
NeuralNetwork -- "1" CostFunction : costFunction >


abstract class CostFunction  {
      {abstract} + calculate(value: Vector, expectedValue: Vector): Vector
      {abstract} + calculate_gradient(value: Vector, expectedValue: Vector): Vector
}

CostFunction <|-- MeanAbsoluteError
CostFunction <|-- MeanSquaredError
CostFunction <|-- CrossEntropy
CostFunction <|-- BinaryCrossEntropy
}



SupervisedModel <|-- BaggingRegularization
' CostFunction --> "0..1" NormPenaltyRegularization : regularizationFunction >


abstract class SupervisedModel {
      {abstract} + train(input: Vector, expectedOutput: Vector, batchFunction: BatchFunction): void
      {abstract} + predict(input: Vector): Vector
}

class NeuralNetwork {
      - layers: Layer[]
      - costFunction: CostFunction
      - learning_rate: float
      - epochs: int
      - regularization_function: NormRegularizationFunction
      - optimizer: Optimizer

      - backprop(output, expected): void
}

abstract class Layer {
      - bias: Vector
      - weights: Matrix
      - units: int
      
      {abstract} + forward(input: Vector): Vector
      {abstract} + backward(gradient: Vector, learningRate: float): Vector
      {abstract} + update_weight(learningRate: float, grads: Vector): void
      {abstract} + update_bias(learningRate: float, grads: Vector): void
}

package ConvolutionLayers {
class ConvolutionLayer

abstract class PoolingLayer
Layer <|-- ConvolutionLayer
Layer <|-- PoolingLayer

Layer <|-- ReshapeLayer

ReshapeLayer <|-- FlattenLayer
ReshapeLayer <|-- UnflattenLayer

PoolingLayer <|-- MaxPooling
PoolingLayer <|-- AvgPooling
}

package ActivationFunctions {
abstract class ActivationFunction {
      {abstract} + calculate(value: Vector): Vector
      {abstract} + calculate_gradient(value: Vector): Vector
}

Layer --> "1" ActivationFunction : activationFunction >

ActivationFunction <|-- ReLU
ActivationFunction <|-- Sigmoid
ActivationFunction <|-- TanH
ActivationFunction <|-- Softmax
}

package Regularization {
class BaggingRegularization {
      + _(models: List[SupervizedModels])
}

BaggingRegularization --> "1..*" SupervisedModel : models >

abstract class NormPenaltyRegularization {
	 {abstract} + calculate(layer: Layer): float
	 {abstract} + calculate_gradientWeight(layer: Layer): Matrix
	 {abstract} + calculate_gradientBiase(layer: Layer): Vector
}

NeuralNetwork --> "0..1" NormPenaltyRegularization: regularizationFunction >

NormPenaltyRegularization <|-- L1WeightDecay
NormPenaltyRegularization <|-- L2WeightDecay

Layer <|-- Dropout
}

package BatchFunctions {
abstract class BatchFunction {
      - inputData: Matrix
      - expectedOutput: Matrix

	 {abstract} + get_batch(): Tuple[Matrix, Matrix]
}

class BatchMode
class MiniBatch

class MiniBatchNormalized {
      {static} - epsilon: float
}

NeuralNetwork --> "1" BatchFunction: batchFunction >

BatchFunction <|-- BatchMode
BatchFunction <|-- MiniBatch
MiniBatch <|-- MiniBatchNormalized
}

package Optimizers {
abstract class Optimizer{
      + calculate_weight(grad: Vector, learningRate: float):
      + calculate_bias(grad: Vector, learningRate: float):
}

NeuralNetwork --> "1" Optimizer

Optimizer <|-- Adam
Optimizer <|-- RMSProp
Optimizer <|-- AdaGrad
Optimizer <|-- SGD
Optimizer <|-- SGDMomentum
}



@enduml