@startuml
!pragma teoz true
skinparam shadowing false
' skinparam monochrome true
' hide footbox
hide empty members

abstract class SupervisedModel {
      {abstract} + train(input: Vector, expectedOutput: Vector): void
      {abstract} + predict(input: Vector): Vector
}

class NeuralNetwork {
      - num_inputs: int
      - num_ouputs: int
      - learning_rate: float
      - iterations: int
      - regularization_rate: float

      + train(input: Vector, expectedOutput: Vector, batch_function: BatchFunction): void
      + backprop(output, expected): Tuple[List[Matrix],List[Vector]]
}

SupervisedModel <|-- NeuralNetwork

abstract class Layer {
      - bias: Vector
      - weights: Matrix
      - units: int
      
      {abstract} + forward(input: Vector): Vector
      {abstract} + backward(gradient: Vector, learning_rate: float): Vector
      {abstract} + update_weight(learning_rate: float, grads: Vector): void
      {abstract} + update_bias(learning_rate: float, grads: Vector): void
}

class ClassicLayer
class ConvolutionLayer
abstract class PoolingLayer

Layer <|-- ClassicLayer
Layer <|-- ConvolutionLayer
Layer <|-- PoolingLayer

class MaxPooling
class AvgPooling

PoolingLayer <|-- MaxPooling
PoolingLayer <|-- AvgPooling

abstract class InitializationFunction {
	 {abstract} + initialize(x: int, y: int): Matrix
}

ClassicLayer --> "1" InitializationFunction: initFunction >

class Random
class He
class Xavier
class Other

InitializationFunction <|-- Random
InitializationFunction <|-- He
InitializationFunction <|-- Xavier
InitializationFunction <|-- Other

abstract class CostFunction {
      {abstract} + calculate(value, expected_value): float
      {abstract} + calculateGradient(value, expected_value): Vector
}

abstract class ActivationFunction {
      {abstract} + calculate(...): Vector
      {abstract} + calculateGradient(...): Vector
}

NeuralNetwork -- "1" CostFunction : costFunction >
NeuralNetwork --> "*" Layer : hiddenLayers >
' NeuralNetwork --> "1" Layer : outputLayer >

ClassicLayer --> "1" ActivationFunction : activationFunction >


CostFunction <|-- MeanAbsoluteError
CostFunction <|-- MeanSquaredError
CostFunction <|-- CrossEntropy

ActivationFunction <|-- ReLU
ActivationFunction <|-- Sigmoid
ActivationFunction <|-- TanH
ActivationFunction <|-- Softmax


abstract class ActivationFunctionRegularizaton {
	 + _(activationFunction: ActivationFunction)
	 {abstract} + regularize(value: Vector): Vector
	 {abstract} + regularizationGradient(value: Vector): Vector
}

class Dropout

ActivationFunction <|-- ActivationFunctionRegularizaton
ActivationFunctionRegularizaton <|-- Dropout

class BaggingRegularization {
      + _(models: List[SupervizedModels])
}

SupervisedModel <|-- BaggingRegularization
BaggingRegularization --> "1..*" SupervisedModel : models >

abstract class NormPenaltyRegularization {
	 {abstract} + calculate(layer: Layer): float
	 {abstract} + calculateGradientWeight(layer: Layer): Matrix
	 {abstract} + calculateGradientBiase(layer: Layer): Vector
}

NeuralNetwork --> "0..1" NormPenaltyRegularization: regularizationFunction >

class L1WeightDecay
class L2WeightDecay

NormPenaltyRegularization <|-- L1WeightDecay
NormPenaltyRegularization <|-- L2WeightDecay
' CostFunction --> "0..1" NormPenaltyRegularization : regularizationFunction >

abstract class BatchFunction {
	 {abstract} + _(input_data: Matrix, expected_output: Matrix): void
	 {abstract} + get_batch(): Tuple[Matrix, Matrix]
}

class BatchMode

class MiniBatch

class MiniBatchNormalized {
      {static} - epsilon: float
}

NeuralNetwork --> "1" BatchFunction: batchFunction >

BatchFunction <|-- BatchMode
BatchFunction <|-- MiniBatch
MiniBatch <|-- MiniBatchNormalized

@enduml